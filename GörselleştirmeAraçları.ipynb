{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_dist(data, var):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    sns.histplot(data=data, x=var, kde=True, ax=ax[0])\n",
    "    sns.boxplot(data=data, x=var, ax=ax[1])\n",
    "    ax[0].set_title(f\"{var} Distribution Histogram\")\n",
    "    ax[1].set_title(f\"{var} Distribution Boxplot\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def cat_dist(data, var):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    df_train[var].value_counts().plot(kind=\"pie\", explode=[0.05 for x in data[var].dropna().unique()], autopct='%1.1f%%', ax=ax[0], shadow=True)\n",
    "    ax[0].set_title(f\"{var} Pie Chart\")\n",
    "    ax[0].set_ylabel('')\n",
    "\n",
    "    count = sns.countplot(x=var, data=df_train, ax=ax[1])\n",
    "    for bar in count.patches:\n",
    "        count.annotate(format(bar.get_height()),\n",
    "            (bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height()), ha='center', va='center',\n",
    "            size=11, xytext=(0, 8),\n",
    "            textcoords='offset points')\n",
    "    ax[1].set_title(f\"{var} Bar Chart\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(20, 8))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i, var in enumerate(num_var+cat_var):\n",
    "    if i < 4:\n",
    "        sns.histplot(data=df_train, x=var, hue=target, kde=True, ax=ax[i])\n",
    "    else:\n",
    "        sns.countplot(data=df_train, x=var, hue=target, ax=ax[i])\n",
    "    \n",
    "    ax[i].set_title(f\"{var}: Survived vs Not Survived\")\n",
    "    \n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for i, pc in enumerate(sorted(df_train[\"Pclass\"].unique())):\n",
    "    sns.histplot(data=df_train[df_train[\"Pclass\"]==pc], x=\"Fare\", hue=target, kde=True, ax=ax[i])\n",
    "    ax[i].set_title(f\"Fare in Pclass {pc} Survival Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [df_train, df_test]\n",
    "for df in data:\n",
    "    df['Relatives'] = df['SibSp'] + df['Parch']\n",
    "    df.loc[df['Relatives'] > 0, 'Alone'] = 1\n",
    "    df.loc[df['Relatives'] == 0, 'Alone'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some variables like Sex, Embarked, and Title are categorical, so we need to encode them first so that it can be used into machine learning models.\n",
    "\n",
    "df_train = pd.get_dummies(df_train, prefix=[\"Sex\", \"Embarked\", \"Title\"])\n",
    "df_test = pd.get_dummies(df_test, prefix=[\"Sex\", \"Embarked\", \"Title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"KNN\": KNeighborsClassifier(), \n",
    "    \"LR\": LogisticRegression(max_iter=1000), \n",
    "    \"DT\": DecisionTreeClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"MLP\": MLPClassifier(max_iter=1000),\n",
    "    \"XGB\": XGBClassifier(),\n",
    "    \"LGBM\": LGBMClassifier()\n",
    "}\n",
    "\n",
    "results = pd.DataFrame(columns=[\"Classifier\", \"Avg_Accuracy\", \"Avg_F1_Score\"])\n",
    "for name, clf in classifiers.items():\n",
    "    model = clf\n",
    "    cv_results = cross_validate(\n",
    "        model, X_train_scaled, y_train, cv=10,\n",
    "        scoring=(['accuracy', 'f1'])\n",
    "    )\n",
    "\n",
    "    results = results.append({\n",
    "        \"Classifier\": name,\n",
    "        \"Avg_Accuracy\": cv_results['test_accuracy'].mean(),\n",
    "        \"Avg_F1_Score\": cv_results['test_f1'].mean()\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "results[\"Avg_Overall\"] = (results[\"Avg_Accuracy\"] + results[\"Avg_F1_Score\"]) / 2\n",
    "results = results.sort_values(\"Avg_Overall\", ascending=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=results, x=\"Avg_Overall\", y=\"Classifier\")\n",
    "plt.title(\"Average Overall CV Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "params = {\n",
    "    \"penalty\": (\"l1\", \"l2\", \"elasticnet\"),\n",
    "    \"tol\": (0.1, 0.01, 0.001, 0.0001),\n",
    "    \"C\": (10.0, 1.0, 0.1, 0.01)\n",
    "}\n",
    "clf = GridSearchCV(lr, params, cv=10)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "print(\"Best hyperparameter:\", clf.best_params_)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
